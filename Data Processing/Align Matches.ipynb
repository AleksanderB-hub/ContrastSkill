{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the pre-training data\n",
    "1. Import Libraries \n",
    "2. Import previously extracted data\n",
    "3. Extract the actual matches\n",
    "4. Align the matches with the tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import json \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the ealier extracted and matched datasets\n",
    "path = \"./Extracted_competencies/extracted.csv\"\n",
    "extracted_matched = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove commas (to ensure clear matchign between lemmatized and unprocessed sentences)\n",
    "extracted_matched['Sentence'] = extracted_matched['Sentence'].apply(lambda x: x.replace(',', ''))\n",
    "extracted_matched['Sentence_lemmatized'] = extracted_matched['Sentence_lemmatized'].apply(lambda x: x.replace(',', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used to extract the actual matched entities for each competence\n",
    "def separate_matches(df):\n",
    "    #Identify skill and knowledge columns\n",
    "    skill_pattern = re.compile(r'^skill_(\\d+)$')\n",
    "    knowledge_pattern = re.compile(r'^knowledge_(\\d+)$')\n",
    "\n",
    "    #Separate skill and knowledge columns\n",
    "    skill_cols = [col for col in df.columns if skill_pattern.match(col)]\n",
    "    skill_fragment_cols = [col for col in df.columns if col.startswith('skill_') and col.endswith('_fragments')]\n",
    "    knowledge_cols = [col for col in df.columns if knowledge_pattern.match(col)]\n",
    "    knowledge_fragment_cols = [col for col in df.columns if col.startswith('knowledge_') and col.endswith('_fragments')]\n",
    "\n",
    "    #Define base columns (not skill or knowledge)\n",
    "    base_cols = [col for col in df.columns if col not in skill_cols + skill_fragment_cols + knowledge_cols + knowledge_fragment_cols]\n",
    "\n",
    "    exploded_rows = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        base_data = row[base_cols]\n",
    "\n",
    "        #Flags to check if competences are present\n",
    "        has_skill = False\n",
    "        has_knowledge = False\n",
    "\n",
    "        #Skills\n",
    "        for skill_col in skill_cols:\n",
    "            skill_value = row[skill_col]\n",
    "            if pd.notna(skill_value):\n",
    "                has_skill = True\n",
    "                skill_num = skill_col.split('_')[1]\n",
    "                fragment_col = f'skill_{skill_num}_fragments'\n",
    "                fragment_value = row.get(fragment_col, pd.NA)\n",
    "                new_row = base_data.copy()\n",
    "                new_row['skill'] = skill_value\n",
    "                new_row['skill_fragments'] = fragment_value\n",
    "                new_row['knowledge'] = pd.NA\n",
    "                new_row['knowledge_fragments'] = pd.NA\n",
    "                exploded_rows.append(new_row)\n",
    "\n",
    "        #Knowledge\n",
    "        for knowledge_col in knowledge_cols:\n",
    "            knowledge_value = row[knowledge_col]\n",
    "            if pd.notna(knowledge_value):\n",
    "                has_knowledge = True\n",
    "                knowledge_num = knowledge_col.split('_')[1]\n",
    "                fragment_col = f'knowledge_{knowledge_num}_fragments'\n",
    "                fragment_value = row.get(fragment_col, pd.NA)\n",
    "                new_row = base_data.copy()\n",
    "                new_row['knowledge'] = knowledge_value\n",
    "                new_row['knowledge_fragments'] = fragment_value\n",
    "                new_row['skill'] = pd.NA\n",
    "                new_row['skill_fragments'] = pd.NA\n",
    "                exploded_rows.append(new_row)\n",
    "\n",
    "        #For the unmatched examples\n",
    "        if not has_skill and not has_knowledge:\n",
    "            new_row = base_data.copy()\n",
    "            new_row['skill'] = pd.NA\n",
    "            new_row['skill_fragments'] = pd.NA\n",
    "            new_row['knowledge'] = pd.NA\n",
    "            new_row['knowledge_fragments'] = pd.NA\n",
    "            exploded_rows.append(new_row)\n",
    "\n",
    "    exploded_df = pd.DataFrame(exploded_rows)\n",
    "\n",
    "    #Reorder columns to match the desired order\n",
    "    columns_order = base_cols + ['skill', 'skill_fragments', 'knowledge', 'knowledge_fragments']\n",
    "    exploded_df = exploded_df[columns_order]\n",
    "    exploded_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return exploded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the stopwords again (to ensure these are marked as relevant if appear within the orginal match)\n",
    "stopset_r = set(stopwords.words('english'))\n",
    "stopset_r.update(['on','approximately', 'approximately', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december', ':', 'love', 'please', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'overtime', '()', '( )', 'bonus',' bonuses', 'Â£' , '?', 'be', 'aberdeen', 'armagh', 'bangor', 'bangor', 'bath', 'belfast', 'birmingham', 'bradford', 'brighton hove', 'bristol', 'cambridge', 'canterbury', 'cardiff', 'carlisle', 'chelmsford', 'chester', 'chichester', 'colchester', 'coventry', 'derby', 'doncaster', 'dundee', 'dunfermline', 'durham', 'edinburgh', 'ely', 'exeter', 'glasgow', 'gloucester', 'hereford', 'inverness', 'kingston upon hull', 'lancaster', 'leeds', 'leicester', 'lichfield', 'lincoln', 'lisburn', 'liverpool', 'london', 'londonderry', 'manchester', 'milton keynes', 'newcastle upon tyne', 'newport', 'newry', 'norwich', 'nottingham', 'oxford', 'perth', 'peterborough', 'plymouth', 'portsmouth', 'preston', 'ripon', 'salford', 'salisbury', 'sheffield', 'southampton', 'southend - on - sea', 'st albans', 'st asaph ', 'llanelwy', 'st davids', 'stirling', 'stoke-on-trent', 'sunderland', 'swansea', 'truro', 'wakefield', 'wells', 'westminster', 'winchester', 'wolverhampton', 'worcester', 'wrexham', 'york', 'bedfordshire', 'berkshire', 'bristol', 'buckinghamshire', 'cambridgeshire', 'cheshire', 'cornwall', 'cumbria', 'derbyshire', 'devon', 'dorset', 'durham', 'east riding of yorkshire', 'east sussex', 'essex', 'gloucestershire', 'greater london', 'greater manchester', 'hampshire', 'herefordshire', 'hertfordshire', 'isle of wight', 'kent', 'lancashire', 'leicestershire', 'lincolnshire', 'merseyside', 'middlesex', 'norfolk', 'north yorkshire', 'northamptonshire', 'northumberland', 'nottinghamshire', 'oxfordshire', 'rutland', 'shropshire', 'somerset', 'south yorkshire', 'staffordshire', 'suffolk', 'surrey', 'tyne and wear', 'warwickshire', 'west midlands', 'west sussex', 'west yorkshire', 'wiltshire', 'worcestershire', 'county antrim', 'county armagh', 'county down', 'county fermanagh', 'county londonderry', 'county tyrone', 'aberdeen', 'aberdeenshire', 'angus', 'argyll and bute', 'clackmannanshire', 'dumfries and galloway', 'dundee', 'east ayrshire', 'east dunbartonshire', 'east lothian', 'east renfrewshire', 'edinburgh', 'falkirk', 'fife', 'glasgow', 'highland', 'inverclyde', 'midlothian', 'moray', 'north ayrshire', 'north lanarkshire', 'orkney', 'perth and kinross', 'renfrewshire', 'scottish borders', 'shetland isles', 'south ayrshire', 'south lanarkshire', 'stirlingshire', 'west dunbartonshire', 'west lothian', 'western isles', 'anglesey / sir fon', 'anglesey/sir fon', 'blaenau gwent', 'bridgend', 'caerphilly', 'cardiff', 'carmarthenshire', 'ceredigion', 'conwy', 'denbighshire', 'flintshire', 'glamorgan', 'gwynedd', 'merthyr tydfil', 'monmouthshire', 'neath port talbot', 'newport', 'newport city', 'pembrokeshire', 'powys', 'rhondda cynon taff', 'swansea', 'torfaen', 'wrexha', 'It', 'iT', 'it'])\n",
    "stopset_r.remove('during')\n",
    "stopset_r.remove('about')\n",
    "stopset_r.remove('other')\n",
    "stopset_r.remove('off')\n",
    "stopset_r.remove('from')\n",
    "stopset_r.remove('down')\n",
    "stopset_r.remove('under')\n",
    "stopset_r.remove('over')\n",
    "stopset_r.remove('own')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to tokenize the sentences\n",
    "lemmatizer_nlp = spacy.load('en_core_web_lg', exclude=['ner', 'parser', 'textcat']) \n",
    "\n",
    "def spacy_tokenize_text(text):\n",
    "\n",
    "    doc = lemmatizer_nlp(text)\n",
    "    \n",
    "    tokenz = [token.text for token in doc]\n",
    "\n",
    "    return tokenz\n",
    "\n",
    "def tokenize_relevant(df):\n",
    "    #Ensure 'Sentence' column is of string type\n",
    "    df['Sentence'] = df['Sentence'].astype(str)\n",
    "    df['Sentence_lemmatized'] = df['Sentence_lemmatized'].astype(str)\n",
    "    df['Tokens'] = df['Tokens'].astype(str)\n",
    "    #Initialize an empty list to store the tokenized sentences\n",
    "    df['Sentence_tokens'] = df['Sentence'].apply(spacy_tokenize_text)\n",
    "    df['Sentence_lemmatized_tokens'] = df['Sentence_lemmatized'].apply(spacy_tokenize_text)\n",
    "    df['Tokens_tokens'] = df['Tokens'].apply(spacy_tokenize_text)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions are used to match the initally extracted matches (lemmatized) with the orignal text in the sentence data\n",
    "def annotate_fragments(df, stop_words_set):\n",
    "    #Defien stopwords for lookup\n",
    "    stop_words_set = set(stop_words_set)\n",
    "\n",
    "    annotations = []\n",
    "    token_masks = []\n",
    "    unmatched_rows = [] \n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        sentence_lemmatized_tokens = row['Sentence_lemmatized_tokens']\n",
    "        skill_fragments = row.get('skill_fragments', None)\n",
    "        knowledge_fragments = row.get('knowledge_fragments', None)\n",
    "        \n",
    "        #Initalize mask of 0s for all tokens in a sequence \n",
    "        mask = [0] * len(sentence_lemmatized_tokens)\n",
    "        \n",
    "        fragments_list = []\n",
    "        if pd.notna(skill_fragments) and skill_fragments.strip():\n",
    "            fragments_list.append(('skill_fragments', skill_fragments))\n",
    "        if pd.notna(knowledge_fragments) and knowledge_fragments.strip():\n",
    "            fragments_list.append(('knowledge_fragments', knowledge_fragments))\n",
    "        \n",
    "        row_annotations = []\n",
    "        unmatched_fragments = []  \n",
    "        \n",
    "        for frag_type, fragments in fragments_list:\n",
    "            #multiple matches are split by \";\" \n",
    "            fragment_groups = [fg.strip() for fg in fragments.split(';') if fg.strip()]\n",
    "            for fragment_group in fragment_groups:\n",
    "                # Matches within the single match are split by \",\"\n",
    "                fragment_items = [fi.strip() for fi in fragment_group.split(',') if fi.strip()]\n",
    "                for fragment in fragment_items:\n",
    "                    fragment_words = fragment.lower().split()\n",
    "                    if not fragment_words:\n",
    "                        continue \n",
    "                    matched_positions_list = match_fragment_in_lemmatized_tokens(\n",
    "                        fragment_words, sentence_lemmatized_tokens, stop_words_set\n",
    "                    )\n",
    "                    if matched_positions_list:\n",
    "                        for positions in matched_positions_list:\n",
    "                            #UIpdate the mask accordingly (lemmatized and raw sentences have the same token structure)\n",
    "                            for pos in positions:\n",
    "                                mask[pos] = 1\n",
    "                            matched_tokens = [sentence_lemmatized_tokens[pos] for pos in positions]\n",
    "                            annotation = {\n",
    "                                'fragment': fragment,\n",
    "                                'matched_tokens': matched_tokens,\n",
    "                                'positions': positions\n",
    "                            }\n",
    "                            row_annotations.append(annotation)\n",
    "                    else:\n",
    "                        #record the unmatched fragments \n",
    "                        unmatched_fragments.append({\n",
    "                            'fragment_type': frag_type,\n",
    "                            'fragment': fragment\n",
    "                        })\n",
    "        annotations.append(row_annotations)\n",
    "        token_masks.append(mask)\n",
    "        \n",
    "        #store the unmatched examples\n",
    "        if unmatched_fragments:\n",
    "            error_row = row.copy()\n",
    "            error_row['unmatched_fragments'] = unmatched_fragments\n",
    "            unmatched_rows.append(error_row)\n",
    "\n",
    "    df['Annotations'] = annotations\n",
    "    df['Token_Mask'] = token_masks\n",
    "    cols_to_keep = ['index', 'JobID', 'Sentence', 'Sentence_tokens', 'skill', 'knowledge', 'Annotations', 'Token_Mask']\n",
    "    df = df[cols_to_keep]\n",
    "\n",
    "    #return the \"error\" dataframe for unmatche examples \n",
    "    error_match_df = pd.DataFrame(unmatched_rows)\n",
    "    return df, error_match_df\n",
    "\n",
    "def match_fragment_in_lemmatized_tokens(fragment_words, lemmatized_tokens, stop_words):\n",
    "    \"\"\"\n",
    "    Match fragment words in lemmatized tokens, allowing stopwords between words.\n",
    "\n",
    "    Parameters:\n",
    "    - fragment_words: List of words in the fragment (lowercased).\n",
    "    - lemmatized_tokens: List of lemmatized tokens from the sentence (including stopwords).\n",
    "    - stop_words: Set of stopwords.\n",
    "\n",
    "    Returns:\n",
    "    - List of positions in lemmatized_tokens where the fragment words occur.\n",
    "    \"\"\"\n",
    "    if not fragment_words:\n",
    "        return [] \n",
    "\n",
    "    matched_positions_list = []\n",
    "    tokens_len = len(lemmatized_tokens)\n",
    "    fragment_len = len(fragment_words)\n",
    "    \n",
    "    for i in range(tokens_len):\n",
    "        #Start with matching the first word (if possible)\n",
    "        if lemmatized_tokens[i].lower() == fragment_words[0]:\n",
    "            positions = [i]\n",
    "            fragment_idx = 1\n",
    "            token_idx = i + 1\n",
    "            while fragment_idx < fragment_len and token_idx < tokens_len:\n",
    "                token_lower = lemmatized_tokens[token_idx].lower()\n",
    "                if token_lower == fragment_words[fragment_idx]:\n",
    "                    #Match next word in fragment\n",
    "                    positions.append(token_idx)\n",
    "                    fragment_idx += 1\n",
    "                    token_idx += 1\n",
    "                elif token_lower in stop_words:\n",
    "                    #Include stopword and continue\n",
    "                    positions.append(token_idx)\n",
    "                    token_idx += 1\n",
    "                else:\n",
    "                    break\n",
    "            if fragment_idx == fragment_len:\n",
    "                matched_positions_list.append(positions)\n",
    "    return matched_positions_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function transforms the dataset into a desired list of dictioanires that can be used with the skill extraction model\n",
    "#The purpose of this function is to create irrelevant masks for each sentence. However, there might be cases where a single sentence was matched with mutiple competencies. In that case we need to account for that \"additional\" relevant tokens when cosntructing a irrelevant mask. \n",
    "def transform_dataset(df):\n",
    "    irrelevant_masks = {}\n",
    "\n",
    "    #Step 1: First, group by 'index' and aggregate the relevant masks\n",
    "    for idx, group in df.groupby('index'):\n",
    "        merged_mask = np.zeros(len(group.iloc[0]['Token_Mask']), dtype=int)\n",
    "        for token_mask in group['Token_Mask']:\n",
    "            merged_mask = np.maximum(merged_mask, token_mask).astype(int).tolist()\n",
    "        irrelevant_masks[idx] = merged_mask\n",
    "\n",
    "    #Step 2: Iterate over the rows and create the list of dictionaries\n",
    "    result_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        competence = row['skill'] if pd.notna(row['skill']) else row['knowledge']\n",
    "        competence_class = 's' if pd.notna(row['skill']) else 'k'\n",
    "\n",
    "        result_dict = {\n",
    "            'index': row['index'],\n",
    "            'JobID': row['JobID'],\n",
    "            'Tokens': row['Sentence_tokens'],\n",
    "            'competence': competence,\n",
    "            'competence_class': competence_class,\n",
    "            'relevant_mask': row['Token_Mask'],\n",
    "            'irrelevant_mask_total': irrelevant_masks[row['index']]\n",
    "        }\n",
    "        result_list.append(result_dict)\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the dataset\n",
    "exploded_matched_tokenized = tokenize_relevant(extracted_matched) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Annotate the dataset\n",
    "exploded_matched_tokenized, annotated_wrong_matched = annotate_fragments(exploded_matched_tokenized, stopset_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect incorrectly matched examples (and remove if neccessary)\n",
    "wrong_index_matched = [i for i in annotated_wrong_matched.index]\n",
    "\n",
    "#Drop incorrectly annotated for matched examples\n",
    "exploded_matched_tokenized = exploded_matched_tokenized.drop(wrong_index_matched)\n",
    "exploded_matched_tokenized = exploded_matched_tokenized.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform into the list of dictionaries \n",
    "matched_dict = transform_dataset(exploded_matched_tokenized)\n",
    "\n",
    "path_to_save = \"./Extracted_competencies\"\n",
    "# Assuming result_list is your list of dictionaries\n",
    "with open(path_to_save+'/matched_dict.json', 'w') as json_file:\n",
    "    json.dump(matched_dict, json_file, indent=4) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
