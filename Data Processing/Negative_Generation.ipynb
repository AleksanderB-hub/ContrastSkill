{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negatives Generation \n",
    "1. Import necessary libraries\n",
    "2. Define the language model for translations\n",
    "3. Clean Data\n",
    "4. Generate back translations \n",
    "5. Process the data following the positive examples preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json \n",
    "import os\n",
    "import torch\n",
    "import regex as re\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'jbochi/madlad400-3b-mt'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import initial negatives\n",
    "green = []\n",
    "file_path = './ContrastSkill/data/Supervised/Green'\n",
    "with open(file_path+'/train.json', 'r') as f:\n",
    "    for line in f:\n",
    "        green.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For extracitng negative examples \n",
    "def extract_negative_examples(data):\n",
    "    \n",
    "    negative_spans = []\n",
    "    \n",
    "    for example in data:\n",
    "        if 'B' not in example['tags_skill']: \n",
    "            sentence = ' '.join(example['tokens'])\n",
    "            negative_spans.append(sentence)\n",
    "    \n",
    "    return negative_spans\n",
    "\n",
    "#For tokenizing\n",
    "def tokenize_sentences(sentences):\n",
    "    \"\"\"\n",
    "    Tokenizes a list of sentences into lists of tokens.\n",
    "    \"\"\"\n",
    "    tokenized_sentences = [sentence.split() for sentence in sentences]\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = extract_negative_examples(green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = pd.DataFrame(negatives, columns=['Sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove HTML tags\n",
    "def remove_html(text):\n",
    "    text = re.sub(r'&nbsp;', ' ', text)\n",
    "    text = re.sub(r'&nbsp\\s+;', ' ', text)\n",
    "    text = re.sub(r'nbsp;', ' ', text)\n",
    "    text = re.sub(r'nbsp\\s+;', ' ', text)\n",
    "    text = re.sub(r'middot;', ' ', text)\n",
    "    text = re.sub(r'middot\\s+;', ' ', text)\n",
    "    text = re.sub(r'ndash;', ' ', text)\n",
    "    text = re.sub(r'ndash\\s+;', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the negative examples \n",
    "negatives = negatives.drop_duplicates()\n",
    "negatives['Sentences'] = negatives['Sentences'].apply(remove_html)\n",
    "negatives['seq_len'] = negatives['Sentences'].apply(lambda x: len(x.split()))\n",
    "negatives = negatives[(negatives['seq_len'] <= 35) & (negatives['seq_len'] > 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store in a list for easier processing \n",
    "negative_sentences = [sentence for sentence in negatives['Sentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_translations(sentence_list, language=None):\n",
    "    back_translated = []\n",
    "    for original_sentence in tqdm(sentence_list, total=len(sentence_list)):\n",
    "        #specify language\n",
    "        if language == 'french':\n",
    "            prefix = '<2fr> '\n",
    "        elif language == 'portuguese':\n",
    "            prefix = '<2pt> '\n",
    "        elif language == 'chinese':\n",
    "            prefix = '<2zh> '\n",
    "        sentence = prefix + original_sentence\n",
    "        input_ids = tokenizer(sentence, return_tensors=\"pt\", max_length=40, truncation=True).input_ids.to(model.device)\n",
    "        outputs = model.generate(input_ids=input_ids, max_new_tokens=40, repetition_penalty=1.5)\n",
    "        translated = tokenizer.decode(outputs[0], skip_special_tokens=True, max_new_tokens=40, repetition_penalty=1.5)\n",
    "        translated = '<2en> ' + translated\n",
    "        #Back translation to english\n",
    "        input_ids_translated = tokenizer(translated, return_tensors=\"pt\", max_length=40, truncation=True).input_ids.to(model.device)\n",
    "        outputs_translated = model.generate(input_ids=input_ids_translated, max_new_tokens=40, repetition_penalty=1.5)\n",
    "        translated_back = tokenizer.decode(outputs_translated[0], skip_special_tokens=True, max_new_tokens=5, repetition_penalty=1.5)\n",
    "        back_translated.append(translated_back)\n",
    "    return back_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Negatives\n",
    "back_neg_ch = generate_translations(negative_sentences, language='chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply simillar pre-processing as for the positive sample and drop duplicates\n",
    "back_neg_ch = [string.replace(',', '') for string in back_neg_ch]\n",
    "back_neg_ch = list(set(back_neg_ch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform into a pandas dataset to match the final structure \n",
    "total_neg_no_dup_df = pd.DataFrame(back_neg_ch, columns=['Sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize to match the desired data structure \n",
    "\n",
    "lemmatizer_nlp = spacy.load('en_core_web_lg', exclude=['ner', 'parser', 'textcat'])  # For Lemmatization only\n",
    "\n",
    "def spacy_tokenize_text(text):\n",
    "    # Apply lemmatization using the lemmatizer model\n",
    "    doc = lemmatizer_nlp(text)\n",
    "    \n",
    "    tokenz = [token.text for token in doc]\n",
    "\n",
    "    return tokenz\n",
    "\n",
    "def tokenize_relevant(df):\n",
    "    #Ensure 'Sentences' column is of string type\n",
    "    df['Sentence'] = df['Sentence'].astype(str)\n",
    "    # Initialize an empty list to store the tokenized sentences\n",
    "    df['Sentence_tokens'] = df['Sentence'].apply(spacy_tokenize_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to add mask for the tokens \n",
    "def add_mask(df):\n",
    "    token_mask = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        sentence_lemmatized_tokens = row['Sentence_tokens']\n",
    "        mask = [1] * len(sentence_lemmatized_tokens)\n",
    "\n",
    "        token_mask.append(mask)\n",
    "\n",
    "    df['relevant_mask'] = token_mask\n",
    "    df['Tokens'] = df['Sentence_tokens']\n",
    "    df = df.drop(columns = ['Sentence', 'Sentence_tokens'])\n",
    "    df = df[['Tokens', 'relevant_mask']]\n",
    "\n",
    "    list_of_dict = df.to_dict(orient='records')\n",
    "    return list_of_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the functions\n",
    "total_neg_no_dup_df_tokenized = tokenize_relevant(total_neg_no_dup_df)\n",
    "negatives_total = add_mask(total_neg_no_dup_df_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to a list of dict\n",
    "with open('./ContrastSkill/Data/Pre-training/negatives_example.json', 'w') as file:\n",
    "    json.dump(negatives_total, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
