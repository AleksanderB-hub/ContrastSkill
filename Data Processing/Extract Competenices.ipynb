{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching Competencies with Job Description data\n",
    "1. Import necessary Libraries\n",
    "2. Import your sentence data from job descriptions\n",
    "3. Lemmatize the dataset\n",
    "4. Remove the stopwords\n",
    "5. Extract the competencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import regex as re\n",
    "import spacy \n",
    "import datetime\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import time as time\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import your job postings data. It must be in a pandas dataframe format. \"id\" is a unique ID, \"Sentence\" are the sentences from job posting data and \"Class\" determines whether the sentence is relevant (i.e., contains competencies) or not-relevant. \n",
    "#Example: \n",
    "data = {\n",
    "    \"id\": [1, 2, 3, 4],\n",
    "    \"Sentence\": [\n",
    "        \"Proficiency in Python, SQL, and data visualization tools such as Tableau or Power BI.\",\n",
    "        \"Strong analytical and problem-solving skills, with a focus on detail.\",\n",
    "        \"Knowledge of cloud computing platforms such as AWS or Google Cloud.\",\n",
    "        \"Embrace and uphold the values and ethos of the company.\",\n",
    "    ],\n",
    "    \"Class\": [1, 1, 1, 0],\n",
    "}\n",
    "\n",
    "example_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize your data\n",
    "lemmatizer_nlp = spacy.load('en_core_web_lg', exclude=['ner', 'parser'])\n",
    "\n",
    "#Replacement dictionary to handle the \"datum\" to \"data\" issue\n",
    "rep_dict_2 = {\"datum\": \"data\", \"Datum\": \"Data\", \"DATUM\": \"DATA\"}\n",
    "\n",
    "def spacylemm_lang(text):\n",
    "    #Apply lemmatization using the lemmatizer model\n",
    "    lemmatizer_sentence = lemmatizer_nlp(text)\n",
    "    \n",
    "    lemmatized_tokens = []\n",
    "    for token in lemmatizer_sentence:\n",
    "        lemma = token.lemma_\n",
    "        # Apply the replacement dictionary for special cases\n",
    "        lemma = rep_dict_2.get(lemma, lemma)\n",
    "        lemmatized_tokens.append(lemma)\n",
    "    \n",
    "    # Join the lemmatized tokens back into a sentence\n",
    "    lemmatized_sentence = \" \".join(lemmatized_tokens)\n",
    "\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define stopwords\n",
    "stopset_r = set(stopwords.words('english'))\n",
    "stopset_r.update(['on','approximately', 'approximately', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december', ':', 'love', 'please', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'overtime', '()', '( )', 'bonus',' bonuses', '£' , '?', 'be', 'aberdeen', 'armagh', 'bangor', 'bangor', 'bath', 'belfast', 'birmingham', 'bradford', 'brighton hove', 'bristol', 'cambridge', 'canterbury', 'cardiff', 'carlisle', 'chelmsford', 'chester', 'chichester', 'colchester', 'coventry', 'derby', 'doncaster', 'dundee', 'dunfermline', 'durham', 'edinburgh', 'ely', 'exeter', 'glasgow', 'gloucester', 'hereford', 'inverness', 'kingston upon hull', 'lancaster', 'leeds', 'leicester', 'lichfield', 'lincoln', 'lisburn', 'liverpool', 'london', 'londonderry', 'manchester', 'milton keynes', 'newcastle upon tyne', 'newport', 'newry', 'norwich', 'nottingham', 'oxford', 'perth', 'peterborough', 'plymouth', 'portsmouth', 'preston', 'ripon', 'salford', 'salisbury', 'sheffield', 'southampton', 'southend - on - sea', 'st albans', 'st asaph ', 'llanelwy', 'st davids', 'stirling', 'stoke-on-trent', 'sunderland', 'swansea', 'truro', 'wakefield', 'wells', 'westminster', 'winchester', 'wolverhampton', 'worcester', 'wrexham', 'york', 'bedfordshire', 'berkshire', 'bristol', 'buckinghamshire', 'cambridgeshire', 'cheshire', 'cornwall', 'cumbria', 'derbyshire', 'devon', 'dorset', 'durham', 'east riding of yorkshire', 'east sussex', 'essex', 'gloucestershire', 'greater london', 'greater manchester', 'hampshire', 'herefordshire', 'hertfordshire', 'isle of wight', 'kent', 'lancashire', 'leicestershire', 'lincolnshire', 'merseyside', 'middlesex', 'norfolk', 'north yorkshire', 'northamptonshire', 'northumberland', 'nottinghamshire', 'oxfordshire', 'rutland', 'shropshire', 'somerset', 'south yorkshire', 'staffordshire', 'suffolk', 'surrey', 'tyne and wear', 'warwickshire', 'west midlands', 'west sussex', 'west yorkshire', 'wiltshire', 'worcestershire', 'county antrim', 'county armagh', 'county down', 'county fermanagh', 'county londonderry', 'county tyrone', 'aberdeen', 'aberdeenshire', 'angus', 'argyll and bute', 'clackmannanshire', 'dumfries and galloway', 'dundee', 'east ayrshire', 'east dunbartonshire', 'east lothian', 'east renfrewshire', 'edinburgh', 'falkirk', 'fife', 'glasgow', 'highland', 'inverclyde', 'midlothian', 'moray', 'north ayrshire', 'north lanarkshire', 'orkney', 'perth and kinross', 'renfrewshire', 'scottish borders', 'shetland isles', 'south ayrshire', 'south lanarkshire', 'stirlingshire', 'west dunbartonshire', 'west lothian', 'western isles', 'anglesey / sir fon', 'anglesey/sir fon', 'blaenau gwent', 'bridgend', 'caerphilly', 'cardiff', 'carmarthenshire', 'ceredigion', 'conwy', 'denbighshire', 'flintshire', 'glamorgan', 'gwynedd', 'merthyr tydfil', 'monmouthshire', 'neath port talbot', 'newport', 'newport city', 'pembrokeshire', 'powys', 'rhondda cynon taff', 'swansea', 'torfaen', 'wrexha', 'It', 'iT', 'it'])\n",
    "stopset_r.remove('during')\n",
    "stopset_r.remove('about')\n",
    "stopset_r.remove('other')\n",
    "stopset_r.remove('off')\n",
    "stopset_r.remove('from')\n",
    "stopset_r.remove('down')\n",
    "stopset_r.remove('under')\n",
    "stopset_r.remove('over')\n",
    "stopset_r.remove('own')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove stopwords\n",
    "def stopwords_rem(text):\n",
    "    stopwords = stopset_r\n",
    "    full_text_ver_0 = re.sub(r'\\bit\\b|\\bIt\\b|\\biT\\b', '', text)\n",
    "    full_text = full_text_ver_0.lower()\n",
    "    words_st_0 = re.sub(r'[^\\w\\s\\/\\-\\.:\\+#]', '', full_text)\n",
    "    words_st_1 = re.sub(r'(?<!C )(?<!C)(?<!C\\+)(?<!C\\+ )(?<!C \\+)(?<!C \\+ )(?<!c )(?<!c)(?<!c\\+)(?<!c\\+ )(?<!c \\+)(?<!c \\+ )\\+', '', words_st_0)\n",
    "    words_st_2 = re.sub(r'(?<!C )(?<!C)(?<!C\\+)(?<!C\\+ )(?<!C \\+)(?<!C \\+ )(?<!c )(?<!c)(?<!c\\+)(?<!c\\+ )(?<!c \\+)(?<!c \\+ )\\#', '', words_st_1).split()\n",
    "    tokens = []\n",
    "    for token in words_st_2:\n",
    "        if token not in stopwords:\n",
    "            tokens.append(token)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def stopwords_remove(new_dataframe_1):\n",
    "\n",
    "    new_dataframe_2 = pd.DataFrame()\n",
    "    \n",
    "    new_dataframe_2['Tokens'] = new_dataframe_1['Sentence_lemmatized'].apply(lambda x: stopwords_rem(x))\n",
    "    new_dataframe_2 = pd.concat([new_dataframe_1, new_dataframe_2], axis=1, join='outer')\n",
    "    \n",
    "    new_dataframe_2 = new_dataframe_2.reset_index(drop=True)\n",
    "    \n",
    "    return new_dataframe_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize data\n",
    "def clean_text(data): \n",
    "    data[['Sentence_lemmatized']] = data['Sentence'].apply(lambda x: pd.Series(spacylemm_lang(x)))\n",
    "    return data \n",
    "\n",
    "example_total = clean_text(example_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords\n",
    "example_total = stopwords_remove(example_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the matching function\n",
    "import sys\n",
    "path_to_your_matching_rules = \"ContrastSkill/Rules\"\n",
    "path = r'C:\\Users\\olekb\\OneDrive\\Desktop\\Codes\\Rules\\extraction_rules.py'\n",
    "sys.path.append(path_to_your_matching_rules)\n",
    "\n",
    "from extraction_rules import knowledge, skill, join_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Skills...: 100%|██████████| 4/4 [00:02<00:00,  1.74it/s]\n",
      "Extracting Knowledge...: 100%|██████████| 4/4 [00:01<00:00,  3.26it/s]\n"
     ]
    }
   ],
   "source": [
    "#Extract the matches\n",
    "skills = skill(example_total)\n",
    "all = knowledge(skills)\n",
    "\n",
    "#Save your extracted matches\n",
    "path_to_save = \"ContrtastSkill/Data/Extracted_competencies\"\n",
    "path_to_save = r'C:\\Users\\olekb\\OneDrive\\Desktop\\Codes\\Data\\Extracted_competencies'\n",
    "os.makedirs(path_to_save, exist_ok=True)\n",
    "all.to_csv(os.path.join(path_to_save, 'extracted.csv'), index = None, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
