{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating similarity between datasets (DICE)\n",
    "1. Import neccesary libraries \n",
    "2. Upload Datasets\n",
    "3. Run Similarity algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload datasets\n",
    "green = []\n",
    "skillspan = []\n",
    "sayfullina = []\n",
    "path = \"./Data/Supervised\"\n",
    "\n",
    "#Green (g)\n",
    "with open(path+'/Green/total_train.json', 'r') as f:\n",
    "    for line in f:\n",
    "        green.append(json.loads(line.strip()))\n",
    "\n",
    "#SkillSpan (ss)\n",
    "with open(path+'/SkillSpan/total_train.json', 'r') as f:\n",
    "    for line in f:\n",
    "        skillspan.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract all spans \n",
    "def extract_relevant_spans(data):\n",
    "    relevant_spans = []\n",
    "    \n",
    "    for entry in data:\n",
    "        tokens = entry['tokens']\n",
    "        tags = entry['tags_skill']\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(tags):\n",
    "            if tags[i] == 'B':\n",
    "                # Start a new span\n",
    "                span_tokens = [tokens[i]]\n",
    "                i += 1\n",
    "                \n",
    "                # Collect following 'I' tags\n",
    "                while i < len(tags) and tags[i] == 'I':\n",
    "                    span_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "                \n",
    "                # Append the span to the list as a single string\n",
    "                relevant_spans.append(' '.join(span_tokens))\n",
    "            else:\n",
    "                i += 1\n",
    "    \n",
    "    # Confirm that all elements are strings\n",
    "    assert all(isinstance(span, str) for span in relevant_spans), \"Not all spans are strings\"\n",
    "\n",
    "    return relevant_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting relevant spans \n",
    "relevant_spans_g = extract_relevant_spans(green)\n",
    "relevant_spans_ss = extract_relevant_spans(skillspan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mesure similarity across spans \n",
    "\n",
    "tokenized_spans_ss = [set(word_tokenize(span.lower())) for span in relevant_spans_ss]\n",
    "tokenized_spans_g = [set(word_tokenize(span.lower())) for span in relevant_spans_g]\n",
    "\n",
    "#Function to calculate the Dice coefficient\n",
    "def dice_coefficient(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    return 2 * intersection / (len(set1) + len(set2))\n",
    "\n",
    "#Initialize a matrix to store the Dice coefficient for each pair of spans\n",
    "dice_matrix = np.zeros((len(tokenized_spans_ss), len(tokenized_spans_g)))\n",
    "\n",
    "#Calculate the Dice coefficient for each pair of spans between the two datasets\n",
    "for i, span_ss in enumerate(tokenized_spans_ss):\n",
    "    for j, span_green in enumerate(tokenized_spans_g):\n",
    "        dice_matrix[i, j] = dice_coefficient(span_ss, span_green)\n",
    "\n",
    "print(\"Pairwise Dice Similarity Matrix:\")\n",
    "print(dice_matrix)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
